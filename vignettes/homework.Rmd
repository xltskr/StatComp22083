---
title: "homework"
author: "22083"
date: "2022/11/25"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{homework}
 %\VignetteEngine{knitr::rmarkdown}
 %\VignetteEncoding{UTF-8}
---

# hw1-2022-09-15

```{r}
library(latex2exp)
library(scales)
library(pander)
```

## Question

Exercises 3.3, 3.7, 3.12, and 3.13 (pages 94-96, Statistical Computating with R).

## Answer

### 3.3

Denote $u=F(x), $ so $u=1-(\frac bx)^a$

 $\quad\quad\because x\geq b>0, a>0$
 
 $\quad\quad\therefore x=b/(1-u)^{\frac 1a}$

$\quad\quad\therefore F^{-1}(u)=b/(1-u)^{\frac 1a}$
```{r}
pareto <- function(a,b) {
  # Generate random numbers with cdf F(x)
  u <- runif(1e4)
  x <- b*(1-u)^(-1/a)
  
  # Draw the histogram of random numbers generated
  hist(x,prob=TRUE,col='pink',main=TeX('Pareto Distribution: $f(x)=\\frac{ab^a}{x^{a+1}}$'))
  legend("top",legend = 'sample',fill='pink',bty="n")
  
  # Draw the density function f(x)
  y <- seq(0, max(x), 0.01)
  lines(y,{a*(b^a)}/{y^{a+1}},lwd=1.5)
  legend("topright",legend = 'theoretical',lwd=1.5,bty="n")
}
pareto(2,2)
```


### 3.7

Suppose $X\sim$Beta$(a,b)$, the density of $X$ is $$f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1},\quad 0<x<1.$$
It can be obtained that $$x_0=\underset{x\in(0,1)}{\arg\max}~f(x)=\frac{a-1}{a+b-2}.$$
Let reference density be $g(x)=\pmb 1_{\{0<x<1\}}$, then $$\frac{f(x)}{g(x)}=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}\leq c=f(x_0).$$

Based on the **acceptance-rejection algorithm**, our algorithm is shown below:

- Generate random numbers $U\sim U(0,1)$ and $Y\sim g(\cdot)$, i.e. $Y\sim U(0,1)$;
- if $U\leq\dfrac{f(Y)}{cg(Y)}$, then accept $Y$ and return $X=Y$; otherwise reject $Y$ and continue.


```{r}
beta <- function(a,b) {
  # Calculate constant c
  x0 <- (a-1)/(a+b-2)
  c <- x0^(a-1)*(1-x0)^(b-1)  # constant in pdf can be ignored
  
  # Generate random numbers with pdf f(x)
  n <- 1e4
  j<-k <- 0
  y <- numeric(n)
  while (k < n) {
    u <- runif(1)
    j <- j + 1
    x <- runif(1) # random variate from g(x)
    if (x^(a-1)*(1-x)^(b-1) / c > u) {
      # accept x
      k <- k + 1
      y[k] <- x
    }
  }
  # Draw the histogram of random numbers generated
  hist(y,prob=TRUE,col='pink',ylim=c(0,2),main=paste('Beta(',a,',',b,')'))
  legend("top",legend = 'sample',fill='pink',bty="n")
  #superimpose the theoretical Beta(3,2) density 
  z<-seq(0,1,0.01)
  lines(z,12*z^2*(1-z),lwd=1.5)
  legend("topright",legend = 'theoretical',lwd=1.5,bty="n")
}
beta(3, 2)
```


### 3.12&13

$\because$ the cdf is $F(y)=1-\big(\frac{\beta}{\beta+y}\big)^2,\quad y\geq 0$

$\quad\quad\therefore f(y)=\frac{r\beta^r}{(\beta+y)^{r+1}},\quad y\geq 0$
```{r}
expgamma <- function(r, beta) {
  # Generate random numbers from the mixture
  n <- 1e4
  x <- rgamma(n, r, beta)
  y <- rexp(n, x)
  hist(x,prob=TRUE,col='pink',main='Exponential-Gamma Mixture')
  legend("top",legend = 'sample',fill='pink',bty="n")
  #superimpose the theoretical Pareto distribution density 
  y<-seq(0,25,0.01)
  lines(y,r*beta^r/((beta+y)^(r+1)),lwd=1.5)
  legend("topright",legend = 'theoretical',lwd=1.5,bty="n")
}
expgamma(4, 2)
```




# hw2-2022-09-23

## Question
  
  Question about fast sorting algorithm and exercises 5.6, 5.7(pages 149-151, Statistical Computing with R)
  
## Answer

### Question about fast sorting algorithm

#### apply the fast sorting algorithm to randomly permuted numbers of 1~n.
```{r}
# function to implement the fast sorting algorithm
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1] # list without the first number
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))} # iteration
}
n<-c(1e4,2e4,4e4,6e4,8e4)
for(i in 1:5){
  test<-sample(1:n[i])
  quick_sort(test) # execute but don't return the sorted list
}
```


#### Calculate computation time averaged over 100 simulations.

```{r}
tt<-matrix(nrow=5,ncol=100)
for(i in 1:5){
  for(j in 1:100){
    test<-sample(1:n[i])
    tt[i,j]<-as.numeric(system.time(quick_sort(test))[1])
  }
}
a<-rowMeans(tt)
# values of a as n varies
rbind(n,a)
```


#### Regress and plot.

```{r}
t<-n*log(n)
model1<-lm(a~t)
plot(t,a,main='Regression',xlab=TeX('$t=nlog(n)$'),ylab=TeX('$a_n$'))
lines(t,fitted(model1),col='red')
```


### 5.6

$\quad\quad$$\theta=E[e^U],\quad U\sim U(0,1),$
$$
\begin{aligned}
     \therefore Cov({\rm e}^U,{\rm e}^{1-U})&=E\big[{\rm e}^U{\rm e}^{1-U}\big]-E\big[{\rm e}^U\big]E\big[\rm e^{1-U}\big]\\
                                          &={\rm e}-\big(\int_0^1{{\rm e}^u}du\big)^2\\
                                          &={\rm e}-({\rm e}-1)^2\\
                                          &=-{\rm e}^2+3{\rm e}-1\\
                                          &\approx-0.2356806
\end{aligned}
$$
$$
\begin{aligned}
     Var({\rm e}^U+{\rm e}^{1-U})&=E\big[\big({\rm e}^U+{\rm e}^{1-U}\big)^2\big]-\big(E\big[{\rm e}^U+{\rm e}^{1-U}\big]\big)^2\\
                                          &=2E\big[{\rm e}^{2U}\big]+2{\rm e}-4\big(E\big[{\rm e}^U\big]\big)^2\\
                                          &={\rm e}^2-1+2{\rm e}-4({\rm e}-1)^2\\
                                          &=-3{\rm e}^2+10{\rm e}-5\\
                                          &\approx0.01564999
\end{aligned}
$$
* Using antithetic variable:
$\hat{\theta}_{AV}=\frac{{\rm e}^U+{\rm e}^{U'}}2=\frac{{\rm e}^U+{\rm e}^{1-U}}2$

$$Var(\hat{\theta}_{AV})=Var\big(\frac{{\rm e}^U+{\rm e}^{1-U}}2\big)=\frac14Var\big({\rm e}^U+{\rm e}^{1-U}\big)\approx0.003912497$$

* Using the simple MC: $\hat{\theta}_{MC}=\frac{{\rm e}^{U_1}+{\rm e}^{U_2}}2$, where $U_1,U_2$ are independent,

$$Var(\hat{\theta}_{MC})=Var\big(\frac{{\rm e}^{U_1}+{\rm e}^{U_2}}2\big)=\frac{Var({\rm e}^U)}2=\frac{E[{\rm e}^{2U}]-\big(E[{\rm e}^U]\big)^2}2=\frac{\frac{{\rm e}^2-1}2-({\rm e}-1)^2}2\approx0.1210178$$

$\therefore$Compared with simple MC, variance of $\hat{\theta}$ using antithetic variates can be reduced $100\%\times (1-0.003912497/0.1210178)=96.77\%$.



### 5.7

```{r}
# generate N simulations
N<-1e4
u<-runif(N/2)
```

* Using antithetic variable:$\because \hat{\theta}_{AV}=\frac1{N/2}\sum\limits_{j=1}^{N/2}\frac{{\rm e}^{U_j}+{\rm e}^{1-U_j}}2=\frac1N\sum\limits_{j=1}^{N/2}\big({\rm e}^{U_j}+{\rm e}^{1-U_j}\big)$,

$\quad\quad$let $g(U)={\rm e}^{U}+{\rm e}^{1-U}$,

$$Var(\hat{\theta}_{AV})=\frac1{2N}Var(g(U_j))=\frac1{2N(N/2-1)}\sum\limits_{i=1}^{N/2}\big(g(U_j)-\overline{g(U)}\big)^2=\frac1{N(N-2)}\sum\limits_{i=1}^{N/2}\big(g(U_j)-\overline{g(U)}\big)^2$$

```{r}
# by the antithetic variate approach:
v1<-var(exp(u)+exp(1-u))/(2*N) # variance of estimate using antithetic variable
v1
```


* Using the simple MC: $\hat{\theta}_{MC}=\frac1N\sum\limits_{i=1}^N {\rm e}^{U_i}$

$$Var(\hat{\theta}_{MC})=\frac1NVar({\rm e}^{U_i})=\frac1{N(N-1)}\sum\limits_{i=1}^{N}\big({\rm e}^{U_i}-\overline{{\rm e}^U}\big)^2$$
```{r}
# by the simple Monte Carlo method:
u<-c(u,runif(N/2))
v2<-var(exp(u))/N # variance of estimate using simple MC
v2
per<-percent((v2-v1)/v2,accuracy = 0.01)
per
```

$\therefore$Compared with simple MC, variance of $\hat{\theta}$ using antithetic variates can be reduced `r  per`.

Compared with the theoretical value from Exercise 5.6 the empirical result is almost equal.


# hw3-2022-09-16

## Question

Exercises 5.13, 5.15(pages 149-151, Statistical Computing with R).

## Answer

### 5.13

$\quad\quad$Due to unbounded interval, we can't use the simple Monte Carlo by comparison. The following two functions are selected. The first one follows Gamma distribution,$\beta=1,\alpha=3$, whose density is 

$$f_1(x)=f(x,\beta,\alpha)=f(x,1,3)=\frac1{\Gamma(3)}x^2e^{-x}=\frac12x^2e^{-x},\quad x>0,$$
The second one follow the exponential distribution but move right by one unit, so that its definitional domain is $(1,\infty)$. The second density is
$$f_2(x)=e^{-(x-1)},\quad x>1.$$
We can make them clear by plotting these densities and ratios for comparison with $g(x)$ in the graph below:

```{r}
x <- seq(1, 5, .01)
f1<-x^2*exp(-x)/2
f2<-exp(1-x)
g<-x^2*exp(-x^2/2)/sqrt(2*pi)
f0<-g
par(mfrow=c(1,2))
plot(x, g, type = "l", main = "Probability density function", ylab = TeX('$f_i(x)'), ylim = c(0,1.5), lwd = 2)
lines(x, f1, lty = 2, lwd = 2,col='pink')
lines(x, f2, lty = 3, lwd = 2,col='blue')
legend("topright", legend = TeX(c('$f_0=g$','$f_1$', '$f_2$')),lty = 1:3, lwd = 2,col = c('black','pink','blue'))
plot(x, g/f0, type = "l", main = expression(g/f_i), ylab= expression(g/f_i),ylim = c(0,3), lwd = 2)
lines(x, g/f1, lty = 2, lwd = 2,col='pink')
lines(x, g/f2, lty = 3, lwd = 2,col='blue')
legend("topleft", legend = TeX(c('$g/f_0=1$','$g/f_1$', '$g/f_2$')),lty = 1:3, lwd = 2,col = c('black','pink','blue'))
```

Then we simulate them to estimate the integration:

```{r}
M<-1e4
theta.hat <- se <- numeric(2) # to save the values of estimate and standard error
g<-function(x){
  x^2*exp(-x^2/2)/sqrt(2*pi)*(x>1)
}
```
```{r}
x<-rgamma(M,shape = 3) # using f1
i<-c(which(x<1))
x[i]<-1  #to catch overflow errors in g(x)
fg<-g(x)/dgamma(x,shape = 3)
theta.hat[1]<-mean(fg)
se[1]<-sd(fg)
```

```{r}
x<-rexp(M,1) # using f2
x<-x+1
fg<-g(x)/exp(1-x)
theta.hat[2]<-mean(fg)
se[2]<-sd(fg)
```
```{r}
rbind(theta.hat, se)
```

As we can be expected, the second importance function produce smaller variance.

<font color=Green>***Explanation:***</font> 


$\quad$ The $\Gamma(\alpha,\beta)=\Gamma(3,1)$ density $f_1$ is supported on the entire positive line, while the integrand $g(x)$ is evaluted on $(1,\infty)$. There are a few values between 0 and 1 (about `r percent(pgamma(1,shape=3),accuracy = 0.01)` (=pgamma(1, shape = 3))) produced, resulting in a bigger variance although we've set them as 1. 


### 5.15

We need to estimate $\theta = \int_0^1\frac{e^{-x}}{1+x^2}dx$.

The best results in Example 5.13 is obtained by the importance function 
$$f_3(x)=e^{-x}/(1-e^{-1}),\quad 0<x<1,$$
corresponding codes are in below:

```{r}
m <- 10000
g <- function(x) exp(-x)/(1+x^2)*(x>0)*(x<1)
f <- function(x){ exp(-x)/(1-exp(-1))*(x>0)*(x<1)}

# f3, inverse transform method
u <- runif(m)
x <- -log(1-u*(1-exp(-1)))
fg <- g(x)/f(x)
theta.im <- mean(fg)
se.im <-sd(fg)
```

In this way $\hat{\theta}_1=$ `r round(theta.im,4)` with $se(\hat{\theta}_1)=$ `r round(se.im, 4)`.

Then we cut the interval as 5-folds: $\left(F^{-1}((j-1)/5),F^{-1}(j/5)\right),j=1,\dots,5$. On the $j^{th}$ subinterval variables are generated from the density

$$\frac{5e^{-x}}{1-e^{-1}}\times1\left(F^{-1}((j-1)/5)<x< F^{-1}(j/5)\right),$$

where $F^{-1}(t)=-\log\left(1-(1-e^{-1})t\right)$.

Consider $U_j\sim U[\frac {j-1}{5},\frac{j}{5}],j=1,\dots,5$ and $\int_0^X\frac{e^{-t}}{1-e^{-1}}dt=U_j$. Then we know X have the density $\frac{5e^{-x}}{1-e^{-1}}\times1\left(F^{-1}((j-1)/5)<x< F^{-1}(j/5)\right).$

Now let we use transformation method to implement stratified importance sampling.

```{r}
k<-5
n<-m/k
theta_s <- var_s <-numeric(k)
for(i in 1:k){
  u <- runif(n,(i-1)/5,i/5)
  x <- -log(1-(1-exp(-1))*u)
  fg <- g(x)/k/f(x)
  theta_s[i]<-mean(fg)
  var_s[i]<-var(fg)
}
```

The $\hat{\theta}$ is 
```{r}
sum(theta_s)
```

And $se(\hat{\theta})$ is
```{r}
sqrt(sum(var_s))
```

which is less than one-tenth of simple importance sampling method ($se(\hat{\theta}_1)=$ `r round(se.im, 4)`).



# hw4-2022-10-09


## Question

Exercises 6.4, 6.8(pages 180-181, Statistical Computing with R) and discussion. 

## Answer

### 6.4

$\quad$If $X$ follows lognormal distribution, $lnX\sim N(\mu,\sigma^2)$, so pdf of $X$ is 
$$
f(x,\mu,\sigma)=
    \begin{cases}
       \frac1{x\sqrt{2\pi}\sigma}\exp{-\frac1{2\sigma^2}(lnx-\mu)^2} & & x>0\\
       0 & & x\leq 0
    \end{cases}
$$
$\quad$Let $Y=lnX, Y_i=lnX_i,i=1,2,...,n$, thus 
$$Y\sim N(\mu,\sigma^2)\quad and \quad {\mu}=\overline{Y}=\frac1n\sum\limits_{i=1}^nY_i$$

$\quad$Set $S^2$ as the variance of sample $Y_i, i=1,.. ,n$, then 

$\quad\because \frac{\sqrt{n}(\hat{\mu}-\mu)}{S}\sim t_{n-1},$

$\quad\therefore100(1-\alpha)\%$ confidence inteval is given by $\quad(\hat{\mu}\pm\frac{S}{\sqrt{n}}t_{n-1}(\alpha/2))$, where $\alpha=0.05$ 

$\quad\therefore P(-t_{n-1}(\alpha/2)<\frac{\sqrt{n}(\hat{\mu}-\mu)}{S}<t_{n-1}(\alpha/2))=0.95$
```{r}
m<-1e4
alpha<-.05
calcCI<-function(mu,sigma,m,n){ # calculate confidence interval
  replicate(m,expr = {
    x<-rlnorm(n,mu,sigma)
    y<-log(x)
    v<-sqrt(var(y)/n)*qt(1-alpha/2,n-1)
    UCL<-mean(y)+v # upper confidence limit
    LCL<-mean(y)-v # lower confidence limit
    return(list(LCL,UCL))
  })
}
```
```{r}
# set mu=1,sigma=1,n=20
LCL<-calcCI(1,1,m,20)[1,]
UCL<-calcCI(1,1,m,20)[2,]
sum(LCL<1&1<UCL)
mean(LCL<1&1<UCL)
```

$\quad$The result is that `r sum(LCL<1&1<UCL)` intervals satisfied ($\mu\in CI$), so the empirical confidence level is `r percent(mean(LCL<1&1<UCL),accuracy = 0.01)` in this experiment.

```{r}
rm(list = ls())
```

### 6.8

```{r}
# The functions of "Count Five" test is copied from the book
maxout <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(max(c(outx, outy)))
}

count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}

F.test <- function(x, y, alpha=0.05){
  S1 <- var(x)
  S2 <- var(y)
  m <- length(x)
  n <- length(y)
  f <- S2/S1
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(f>qf(1-alpha/2,df1 = n-1,df2 = m-1)||
                           f<qf(alpha/2,df1 = n-1,df2 = m-1)))
}
```

Then we write functions to compute the empirical power of "Count Five" test and F test.

```{r}
power_count5test <- function(m, n1, n2, sigma1, sigma2){
  mean(replicate(m, expr={
    x <- rnorm(n1, 0, sigma1)
    y <- rnorm(n2, 0, sigma2)
    count5test(x, y)
  }))
}

power_F.test <- function(m, n1, n2, sigma1, sigma2){
  mean(replicate(m, expr = {
    x <- rnorm(n1, 0, sigma1)
    y <- rnorm(n2, 0, sigma2)
    F.test(x, y, alpha = 0.055)
  }))
}
```

Now we compute the powers of the two tests under different sample sizes, that is $n_1=n_2=20,100,1000$, and we summarize the results in the table below.

```{r}
set.seed(0)
m <- 1e4
# generate samples under H1 to estimate power
sigma1 <- 1
sigma2 <- 1.5
result1 <- numeric(3)
result2 <- numeric(3)
n <- c(20,100,1000)

for(i in 1:3){
  result1[i] <- power_count5test(m, n1=n[i], n2=n[i], sigma1, sigma2)
  result2[i] <- power_F.test(m, n1=n[i], n2=n[i], sigma1, sigma2)
}


pander::pander(data.frame("size"=c(20,100,200),"count five test"=result1,
                          "F test"=result2))
```

From the table we can see that the power of F test is higher than the power of "Count Five" test when the sample is normal distributed.

Finally, we clean the memory of the variables.

```{r}
rm(list = ls())
```



# hw5-2022-10-14

```{r, eval=TRUE,message=FALSE}
# packages will be used
library(boot)
library(bootstrap)
```

## Question

Exercises 7.4, 7.5, 7.A(pages 212-213, Statistical Computing with R).

## Answer

### 7.4
```{r}
data(aircondit, package = "boot")
# we can see the structure of the data set 'aircondit'
aircondit
```

$\quad$Due to the assumption that the times between failures follow an exponential model $Exp(\lambda)$, thus
 $$
f(x,\lambda)=
    \begin{cases}
       \lambda e^{-\lambda x}& & x>0\\
       0 & & x\leq 0
    \end{cases}
$$
We can deduct from the probability density function that $EX=\frac1\lambda$, and the following is the process of obtaining the MLE of $\lambda$: 

$\quad$Denote $\textbf{X} = (X_1,...,X_n)$ as a sample from exponential distribution $Exp(\lambda)$, so the likelihood function, also the distribution of $\textbf{X}$ is 
$$L(\lambda,\textbf{x})=\lambda^n\rm{exp}\{-\lambda\sum\limits_{i=1}^nx_i\}$$
Then the log-likelihood function is:
$$l(\lambda,\textbf{x})=n\rm{log}(\lambda)-\lambda\sum\limits_{i=1}^nx_i$$
$$\frac{\partial{l(\lambda,\textbf{x})}}{\partial{\lambda}}=\frac n\lambda-\sum\limits_{i=1}^nx_i$$
Therefore the MLE of $\lambda$ is:
$$\hat{\lambda}^*=\frac1{\overline{X}}=\frac n{\sum\limits_{i=1}^nx_i}$$
```{r}
# the hazard rate lambda
hr<-function(dat,ind){
  1/mean(dat[ind,])
}
# to estimate the bias and standard error of the estimate
boot(aircondit,statistic = hr,R=1e4)
```


### 7.5

```{r}
# the mean time 1/lambda
rhr<-function(dat,ind){ # reciprocal of hazard rate
  mean(dat[ind,])
}
boot.obj=boot(aircondit,rhr,R=1e4)
# to compute 95% bootstrap confidence intervals by the standard normal, basic, percentile, and BCa methods
boot.ci(boot.obj,type=c("norm","basic","perc","bca")) 
```

### 7.A

```{r}
mu<-1
sigma<-1
n<-10
m<-1e4
boot.mean <- function(x,i) mean(x[i])
ci.norm<-ci.basic<-ci.perc<-matrix(NA,m,2)
for(i in 1:m){
  U<-rnorm(n,mu,sigma) # Sample from a normal population
  de <- boot(data=U,statistic=boot.mean, R = 2000)
  ci <- boot.ci(de,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3]
  ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5]
}
```


```{r}
cat('The estimates of the coverage probabilities are below:\n norm =',mean(ci.norm[,1]<=mu & ci.norm[,2]>=mu),
',basic =',mean(ci.basic[,1]<=mu & ci.basic[,2]>=mu),
',perc =',mean(ci.perc[,1]<=mu & ci.perc[,2]>=mu))
```
```{r}
# the proportion of times that the confidence intervals miss on the left or right
left<-c(mean(ci.norm[,1]>mu),mean(ci.basic[,1]>mu),mean(ci.perc[,1]>mu))
right<-c(mean(ci.norm[,2]<mu),mean(ci.basic[,2]<mu),mean(ci.perc[,2]<mu))
pt<-cbind(left,right)
row.names(pt)<-c('normal','basic','percentile')
pt
```


# hw6-2022-10-21

## Question

Exercises 7.8, 7.11, 8.2(pages 212-213, 242, Statistical Computing with R).

## Answer

### 7.8
```{r}
bias_se.jack <- function(scor){
  #'*Compute original theta.hat*
  scor.cov <- cov(scor)
  ev <- eigen(scor.cov)$values
  theta.hat <- max(ev)/sum(ev)
  
  #'*Define function for each jackknife estimate*
  jack.scor <- function(scor,i){
    d.scor <- scor[-i,]
    d.scor.cov <- cov(d.scor)
    d.ev <- eigen(d.scor.cov)$values
    max(d.ev)/sum(d.ev)
  }
  
  #'*Iteration*
  n <- nrow(scor)
  theta.jack<- sapply(1:n,jack.scor,scor=scor)
  
  #'*Return list containing jackknife bias & se*
  list(
  bias.jack = (n-1)*(mean(theta.jack)-theta.hat),
  se.jack = sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2)))
}

print(bias_se.jack(scor))

detach(package:bootstrap)
rm(list=ls())
```



### 7.11

```{r}
library(DAAG)
attach(ironslag)

model.validation <- function(y,x,valid.num){
  
  #'*Step 1:Define a function that computes the sum of error square for different model & validation points(could be leave-k-out)*
  validerror <- function(y,x,index,fun){
    #'*Delete validation points from sample*
    y_d <- y[-index]
    x_d <- x[-index]
    #'*Choose model according to fun*
    if(fun=="linear"){
      J <- lm(y_d ~ x_d)
      yhat <- J$coef[1] + J$coef[2] * x[index]
    }
    else if(fun=="quadratic"){
      J <- lm(y_d ~ x_d + I(x_d^2))
      yhat <- J$coef[1] + J$coef[2] * x[index] + J$coef[3] * x[index]^2
    }
    else if(fun=="exponential"){
      J <- lm(log(y_d) ~ x_d)
      yhat <- exp(J$coef[1] + J$coef[2] * x[index])
    }
    else if(fun=="log-log"){
      J <- lm(log(y_d) ~ log(x_d))
      yhat <- exp(J$coef[1] + J$coef[2] * log(x[index]))
    }
    else{
      print("Error!Invalid argument.")
    }
    e <- sum((y[index] - yhat)^2)
  }
  
  #'*Step 2:leave k out*
  n <- length(y)
  num <- c(1:n)
  
  #'*Obtain all combinations of 2 from n*
  valid.index <- combn(num,valid.num)
  
  #'*The following sapply function uses dataframe instead of matrix*
  valid.index <- as.data.frame(valid.index)
  
  fun_names <- c("linear","quadratic","exponential","log-log")
  
  for(name in fun_names){
    #'*For each column of dataframe apply validerror function resulting a vector*
    er <- sapply(valid.index,validerror,y=y,x=x,fun=name)
    #'*For each name iteration,assign a different name to er*
    assign(paste0(name,".e"),er)
  }
  
  list(
    `linear validation error`=mean(linear.e),
    `quadratic validation error`=mean(quadratic.e),
    `exponential validation error`=mean(exponential.e),
    `log-log validation error`=mean(`log-log.e`)
  )
}

y <- magnetic
x <- chemical

print(model.validation(y,x,2))
```

It shows that quadratic model has the least leave-two-out error in consistent with leave-one-out validation.

For leave-one-out error and plots of each models,please refer to book page 209 ~
page 211.


```{r}
detach(package:DAAG,ironslag)
rm(list=ls())
```

### 8.2
```{r}
spcor.compare<- function(z){
  #'*Function for each permutation statistic*
  p.spcor <- function(z,ix){
    x <- z[,1]
    y <- z[ix,2]
    return(cor.test(x,y,method="spearman",exact = FALSE)$estimate)
  }
  #'*Use boot to sample permutation statistics*
  obj <- boot(data=z,statistic = p.spcor,R=10000,sim="permutation")
  ts <- c(obj$t0,obj$t)
  
  list(
    #'*p-value of spearman's test using cor.test*
    pvalue = cor.test(x, y, method = "spearman", exact = FALSE)$p.value,
    #'*p-value of spearman's test using permutation*
    p.pvalue = mean(abs(ts)>=abs(ts[1]))
  )
}

#'*Use built-in data iris*
x <- iris[1:50,1]
y <- iris[1:50,3]
z <- cbind(x,y)

print(spcor.compare(z))

detach(package:boot)
rm(list=ls())
```

# hw7-2022-10-24

## Question

Exercises 9.4, 9.7(pages 212-213, 242, Statistical Computing with R)

## Answer

### 9.4
Implementation of a random walk Metropolis sampler for this problem is as follows.

- Set $g(\cdot\mid X)$ to the density of $N(X,\sigma^2)$.
- Generate or initialize $X_1$.
- Repeat for $t=2,\dots,N$:
  - Generate $Y$ from $N(X_{t-1},\sigma^2)$.
  - Generate $U$ from $U(0,1)$.
  - Compute accept probability $\alpha(X_{t-1},Y)=\dfrac{f(Y)}{f(X_{t-1})}=\dfrac{\text{e}^{-|Y|}}{\text{e}^{-|X_{t-1}|}}=\text{e}^{|X_{t-1}|-|Y|}$.
  - If $U\leq\alpha(X_{t-1},Y)$, accept $Y$ and set $X_t=Y$, otherwise set $X_t=X_{t-1}$.
  - Increment $t$, and back to the first step in loop.

```{r}
rl.metropolis <- function(sigma, x0, N) {
  # sigma: sd of proposal distribution N(xt,sigma^2)
  # x0: initial value
  # N: length of chain
  
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0  # to calculate acceptance rate
  for (t in 2:N) {
    y <- rnorm(1, x[t-1], sigma)
    if (u[t] <= exp(abs(x[t-1]) - abs(y))) { x[t] <- y; k <- k + 1 }
    else { x[t] <- x[t-1] }
  }
  return(list(mc = x, acc.prob = k / N))
}

N <- 10000
b <- 1000
k <- 4
sigma <- c(0.5, 1, 4, 16)
x0 <- c(-5, -2, 2, 5)
X <- matrix(nrow = k, ncol = N)
acc.prob <- numeric(k)
for (i in 1:k) {
  rl <- rl.metropolis(sigma[i], x0[i], N)
  X[i, ] <- rl$mc
  acc.prob[i] <- rl$acc.prob
}
acc.prob
```

As we can see, only the acceptance rate of second chain is much more favorable. Next we draw the sample path for each chain.

```{r fig.height=8, fig.width=8}
par(mfrow = c(2, 2))
for (i in 1:k) {
  plot(X[i,], type = "l", xlab = bquote(sigma == .(sigma[i])),
       ylab = "X", ylim = range(X[i,]))
}
```

Next we plot the histogram with the true density besides.

```{r fig.height=8, fig.width=8}
par(mfrow = c(2, 2))
x <- seq(-6, 6, 0.01)
fx <- exp(-abs(x)) / 2
for (i in 1:k) {
  hist(X[i, -(1:b)], breaks = "Scott", freq = FALSE, main = "",
       xlab = bquote(sigma == .(sigma[i])), xlim = c(-6, 6), ylim = c(0, 0.5),)
  lines(x, fx, col = 2, lty = 2)
}
```

From the plots above, we can draw to the same conclusion to the previous part, that is, the second chain is more suitable. Next, we compare the quantiles.

```{r}
z <- rexp(100, 1)
z <- c(-rev(z), z) # generate laplace random numbers
p <- c(0.05, seq(0.1, 0.9, 0.1), 0.95)
Q <- quantile(z, p)
mc <- X[, -(1:b)]
Qmc <- apply(mc, 1, function(x) quantile(x, p))
QQ <- data.frame(round(cbind(Q, Qmc), 3))
names(QQ) <- c('True', 'sigma=0.5', 'sigma=1', 'sigma=4', 'sigma=16')
knitr::kable(QQ)
```

As we can see, the quantiles of the second or third chain are close to the true quantiles of standard Laplace distribution.

---

Finally, we use the Gelman-Rubin method to monitor convergence of the chain. Suppose we are interested in the mean, i.e. $\phi_{it}$ denotes the sample mean until the $t$-th replicates in the $i$-th chain.

```{r}
Gelman.Rubin <- function(phi) {
  phi <- as.matrix(phi)
  k <- nrow(phi); n <- ncol(phi)
  phi.means <- rowMeans(phi)
  B <- n * var(phi.means)
  phi.w <- apply(phi, 1, var)
  W <- mean(phi.w)
  v.hat <- W * (n - 1) / n + B / n
  r.hat <- v.hat / W
  return(r.hat)
}

# ergodic mean plot
phi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(phi)) {
  phi[i,] <- phi[i,] / (1:ncol(phi))
}
for (i in 1:k) {
  if (i == 1) {
    plot((b+1):N, phi[i, (b+1):N], ylim = c(-0.5, 0.5),
         type = "l", xlab = 'Index', ylab = bquote(phi))
  } else { lines(phi[i, (b+1):N], col = i) }
}

# plot of R_hat
rhat <- rep(0, N)
for (j in (b+1):N) {
  rhat[j] <- Gelman.Rubin(phi[, 1:j])
}
plot(rhat[(b+1):N], type = "l", xlab = "", ylab = "R")
abline(h = 1.2, lty = 2)
```

Either from the ergodic mean plot, or from the plot of $\hat{R}$, we could know the chain has converged.


### 9.7

To generate a bivariate normal chain $(X_t,Y_t)$ from  $N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$, we notice that
\begin{align*}
X\mid Y=y&\sim N\left(\mu_1+\rho\dfrac{\sigma_1}{\sigma_2}(y-\mu_2),(1-\rho^2)\sigma_1^2\right),\\
Y\mid X=x&\sim N\left(\mu_2+\rho\dfrac{\sigma_2}{\sigma_1}(x-\mu_1),(1-\rho^2)\sigma_2^2\right).
\end{align*}

So we implement the Gibbs sampler below and plot the generated sample aftering discarding a suitable burn-in sample.

- Set initial value $X_1$, $Y_1$.
- Repeat for $t=2,\dots,N$:
  - Generate $X_t\sim f_{X|Y}(x\mid Y_{t-1})$.
  - Generate $Y_t\sim f_{Y|X}(y\mid X_t)$.
  - Increment $t$, and back to the first step in the loop.

```{r}
rbn.metropolis <- function(mu, sigma, rho, initial, N) {
  # mu, sigma, rho: parameter of bivariate normal distribution.
  # initial: initial value
  # N: length of chain
  
  X <- Y <- numeric(N)
  s <- sqrt(1 - rho^2) * sigma
  X[1] <- initial[1]; Y[1] <- initial[2]
  for (i in 2:N) {
    y <- Y[i-1]
    m1 <- mu[1] + rho * (y - mu[2]) * sigma[1] / sigma[2]
    X[i] <- rnorm(1, m1, s[1])
    x <- X[i]
    m2 <- mu[2] + rho * (x - mu[1]) * sigma[2] / sigma[1]
    Y[i] <- rnorm(1, m2, s[2])
  }
  return(list(X = X, Y = Y))
}

N <- 10000
b <- 1000
rho <- 0.9
mu <- c(0, 0)
sigma <- c(1, 1)
XY <- rbn.metropolis(mu, sigma, rho, mu, N)
X <- XY$X[-(1:b)]; Y <- XY$Y[-(1:b)]
plot(X, Y, xlab = bquote(X[t]), ylab = bquote(Y[t]),
     main = "", cex = 0.5, ylim = range(Y))
cov(cbind(X, Y))
```

From the result above, we can conclude that the chain has converged for the sample covariance matrix is close to the truth. Next, we use different initial values to monitor the convergence of two chains (i.e. $X_t$ and $Y_t$), with $\phi_{it}$ denotes the sample mean until the $t$-th replicates in the $i$-th cain, respectively. Notice that we calculate the sample mean of $X_tY_t$ for monitering the convergence of covariance.

```{r fig.height=4, fig.width=9}
k <- 4
x0 <- matrix(c(2,2,-2,-2,4,-4,-4,4), nrow = 2, ncol = k)
Xmc <- Ymc <- XYmc <- matrix(0, nrow = k, ncol = N)
for (i in 1:k) {
  XY <- rbn.metropolis(mu, sigma, rho, x0[,i], N)
  Xmc[i,] <- XY$X; Ymc[i,] <- XY$Y
  XYmc[i,] <- Xmc[i,] * Ymc[i,]
}

# ergodic mean plot
cal_phi <- function(X) {
  phi <- t(apply(X, 1, cumsum))
  for (i in 1:nrow(phi)) {
    phi[i,] <- phi[i,] / (1:ncol(phi))
  }
  return(phi)
}
phiX <- cal_phi(Xmc)
phiY <- cal_phi(Ymc)
phiXY <- cal_phi(XYmc)

plot_erg_mean <- function(phi, rg) {
  for (i in 1:k) {
    if (i == 1) {
      plot((b+1):N, phi[i, (b+1):N], type = "l", ylim = rg,
           xlab = "Index", ylab = bquote(phi))
    }
    else { lines(phi[i, (b+1):N], col = i) }
  }
}
par(mfrow = c(1, 3))
plot_erg_mean(phiX, rg = c(-0.5, 0.5))
plot_erg_mean(phiY, rg = c(-0.5, 0.5))
plot_erg_mean(phiXY, rg = c(0.7, 1.1))
```

```{r fig.height=4, fig.width=9}
Gelman.Rubin <- function(phi) {
  phi <- as.matrix(phi)
  k <- nrow(phi); n <- ncol(phi)
  phi.means <- rowMeans(phi)
  B <- n * var(phi.means)
  phi.w <- apply(phi, 1, var)
  W <- mean(phi.w)
  v.hat <- W * (n - 1) / n + B / n
  r.hat <- v.hat / W
  return(r.hat)
}

# plot of R_hat
plot_R_hat <- function(phi) {
  rhat <- rep(0, N)
  for (j in (b+1):N) {
    rhat[j] <- Gelman.Rubin(phi[, 1:j])
  }
  plot(rhat[(b+1):N], type = "l", xlab = "", ylab = "R", ylim = c(1, 1.25))
  abline(h = 1.2, lty = 2)
}
par(mfrow = c(1, 3))
plot_R_hat(phiX)
plot_R_hat(phiY)
plot_R_hat(phiXY)
```
Either from the ergodic mean plot, or from the plot of $\hat{R}$, we could know the chain has converged.

---

Finally, we fit the simple linear regression model.

```{r comment = ''}
lm.fit <- lm(Y ~ X)
summary(lm.fit)
```

The coefficients of the fitted model $`r lm.fit$coef[2]`$ is close to the true value $0.9$. Then we check the residuals of the model for normality and constant variance. Theoritically, the variance of $e$ is $$\textsf{Var}(e)=\textsf{Var}(Y-0.9X)=\textsf{Var}(Y)+0.9^2\textsf{Var}(X)-2\times0.9\textsf{Cov}(X,Y)=0.19.$$

```{r fig.height=5, fig.width=8}
par(mfrow = c(1, 2))
e <- lm.fit$residuals
qx <- seq(-2, 2, 0.01)
hist(e, breaks = "Scott", freq = FALSE, main = "", xlim = c(-2, 2), ylim = c(0, 1))
lines(qx, dnorm(qx, 0, sqrt(0.19)), col = 2, lwd = 1.5)
qqnorm(e)
qqline(e, col = 2, lwd = 2, lty = 2)
```



# hw8-2022-11-04

## Question

Two class work.

## Answer

### Mediating effects

Testing the mediating effects. Consider the models 
$$M=a_M+\alpha X+e_M,$$
$$Y=a_Y+\beta M+\gamma X+e_Y,$$
$$e_M,e_Y \sim N(0,1).$$
And $e_M,e_Y$ are independent. 

The hypothesis testing is 
$$H_0:\alpha\beta=0\quad H_1:\alpha\beta\neq0.$$

The test statistics is
$$T=\frac{\hat{\alpha}\hat{\beta}}{\hat{se}(\hat{\alpha}\hat{\beta})}=\frac{\hat{\alpha}\hat{\beta}}{\sqrt{\hat{\alpha}^2\hat{s}_\beta^2+\hat{\beta}^2\hat{s}_\alpha^2}}$$
while $s_\alpha$ and $s_\beta$ are the standard deviations of $\alpha$ and $\beta$ respectively. 

We can implement the permutation test under the following conditions:

* Condition $1$: $\alpha=0,\beta\neq0$. In this situation, $X$ and $M$ are independent, which means that any permutation of $X$ and $M$ are independent;
* Condition $2$: $\beta=0,\alpha\neq0$. In this situation, $M$ and $Y$ are independent, which means that any permutation of $M$ and $Y$ are independent;
* Condition $3$: $\alpha=0,\beta=0$. In this situation, 
  + $M$ and $X$ are independent;
  + $M$ and $Y$ are independent.

Set the real values are

* $\alpha=0,\beta=0,\gamma=1,a_M=0.5,a_Y=1$
* $\alpha=0,\beta=1,\gamma=1,a_M=0.5,a_Y=1$
* $\alpha=1,\beta=0,\gamma=1,a_M=0.5,a_Y=1$

Then implement permutation test for each situation of parameter under the three conditions above.

```{r setup11, fig.height=10, fig.width=10, echo=T, eval=T}
# The function to generate the random sample
RSample <- function(n,alpha,beta){
  X <- runif(n,10,20)
  gamma <- 1;aM <- 0.5;aY <- 1
  M <- aM+alpha*X+rnorm(n)
  Y <- aY+beta*M+gamma*X+rnorm(n)
  return(list(X,M,Y))
}

# The function of test statistics computation
Ttest <- function(X,M,Y){
  fit1 <- summary(lm(M~X))
  fit2 <- summary(lm(Y~X+M))
  a <- fit1$coefficients[2,1]
  sea <- fit1$coefficients[2,2]
  b <- fit2$coefficients[3,1]
  seb <- fit2$coefficients[3,2]
  return(a*b/((a*seb)^2+(b*sea)^2)^0.5)
}

# The function to implement the test hypothesis
Imptest <- function(N,n,X,M,Y,T0){
  T1 <- T2 <- T3 <- numeric(N)
  # Condition 1
  for(i in 1:N){
    n1 <- sample(1:n, size=n, replace=FALSE)
    n2 <- sample(1:n, size=n, replace=FALSE)
    X1 <- X[n1];M1 <- M[n2];Y1 <- Y[n2]
    T1[i] <- Ttest(X1,M1,Y1)
  }
  # Condition 2
  for(i in 1:N){
    n1 <- sample(1:n, size = n, replace = FALSE)
    n2 <- sample(1:n, size = n, replace = FALSE)
    X2 <- X[n1];M2 <- M[n1];Y2 <- Y[n2]
    T2[i] <- Ttest(X2,M2,Y2)
  }
  # Condition 3
  for(i in 1:N){
    n1 <- sample(1:n, size = n, replace = FALSE)
    n2 <- sample(1:n, size = n, replace = FALSE)
    M3 <- M[n1];X3 <- X[n2];Y3 <- Y[n2]
    T3[i] <- Ttest(X3,M3,Y3)
  }
  # The p-value of Condition1
  p1 <- mean(abs(c(T0,T1))>abs(T0))
  # The p-value of Condition2
  p2 <- mean(abs(c(T0,T2))>abs(T0))
  # The p-value of Condition3
  p3 <- mean(abs(c(T0,T3))>abs(T0))
  return(c(p1,p2,p3))
}

N <- 1000 # The number of simulation
n <- 100 # The number of random sample
T0 <- numeric(3)
p <- matrix(0,3,3)
# The real values of parameters
alpha <- c(0,0,1);beta <- c(0,1,0)

for(i in 1:3){
  result <- RSample(n,alpha[i],beta[i])
  X <- result[[1]]
  M <- result[[2]]
  Y <- result[[3]]
  # The original value of test statistics
  T0[i] <- Ttest(X,M,Y)
  p[i,] <- Imptest(N,n,X,M,Y,T0[i])
}
```

Output the table of p-values for the permutation tests above.

```{r setup12, fig.height=10, fig.width=10, echo=T, eval=T}
# Result reporting
colnames(p) <- c("Condition 1","Condition 2","Condition 3")
rownames(p) <- c("alpha=0,beta=0","alpha=0,beta=1","alpha=1,beta=0")
p

# Clean the memory of the variables
rm(list=ls())
```

### Expit model


```{r}
# input: N,b1,b2,b3,f0
# output:alpha
g<-function(N,b1,b2,b3,f0){
  # generate data
  x1<-rpois(N,1)
  x2<-rexp(N,1)
  x3<-rbinom(N,1,0.5)
  p<-function(alpha) mean(1/(exp(-(alpha+b1*x1+b2*x2+b3*x3))))-f0 
  s<-uniroot(p,c(-15,0))
  ss<-round(unlist(s),5)[1]
  return(ss)
}
```

```{r}
N<-1e6
b1<-0
b2<-1
b3<--1
f0<-c(0.1,0.01,0.001,0.0001)
alp<-numeric(length(f0))
for(i in 1:length(f0)){
  alp[i]<-g(N,b1,b2,b3,f0[i])
}
cbind(f0=f0,alpha=alp)
```

```{r}
# f0 vs alpha scatter plot
plot(f0,alp,pch=16,xlab=expression(f[0]),ylab = expression(alpha))
```


# hw9-2022-11-11

## Question

One class work, 2.1.3 Exercise 4, 5;2.3.1 Exercise 1, 2; 2.4.5 Exercise 1, 2, 3 (Advanced in R)


## Answer

### EM algorithm

```{r}
# Estimate $\lambda$ by directly maximizing likelihood function of observed data:
u<-c(11,8,27,13,16,0,23,10,24,2)
v<-u+1 # observed intervals
l_o <- function(lam){ # the obseved data likelihood function
  return(sum((v*exp(-lam*v)-u*exp(-lam*u))/(exp(-lam*u)-exp(-lam*v))))
}
round(uniroot(l_o,c(0.0001,0.5))$root,5)
```

```{r}
# Estimate $\lambda$ by using EM algorithm:
EM<-function(lam0,u,v,N,eps){ # lam0: initial value; N: max_iterations
  i<-1
  lam1<-5
  lam2<-lam0
  n<-length(u)
  while(abs(lam1-lam2)>=eps){
    lam1<-lam2
    lam2<-n/sum(((lam1*u+1)*exp(-lam1*u)-(lam1*v+1)*exp(-lam1*v))/(lam1*(exp(-lam1*u)-exp(-lam1*v))))
    if(i==N) break
    i<-i+1
  }
  return(round(lam2,5))
}
EM(0.5,u,v,1e5,1e-3)
```



# hw10-2022-11-18
```{r}
# packages that will be used
library(MASS)
library(Rcpp)
library(microbenchmark)
```

## Question

Exercises 2 on page 204,Exercises 1 on page 213 (Advanced R) and Gibbs Sampler.

## Answer

### 1
```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
scale02<-function(x){
  data.frame(lapply(x, function(y) if (is.numeric(y)) scale01(y) else y))
}
```

### Gibbs sampler


```{r}
N<-5e3 # length of chain
burn<-1e3 # burn-in length
mu1<-mu2<-0 # zero means
sigma1<-sigma2<-1 # unit standard deviations
rho<-0.9 #correlation
sourceCpp('../src/gibbsC.cpp')
gibbsc<-gibbsC(0,0,1,1,0.9,N)[(burn+1):N,]
plot(gibbsc,cex=0.5,main="Generated with Rcpp",xlab = 'Xt',ylab = 'Yt')
```


```{r}
# Gibbs sampler to generate a bivariate normal chain
gibbsR<-function(mu1,mu2,sigma1,sigma2,rho,N){
  X<-matrix(0,N,2) # for storage
  s1<-sqrt(1-rho^2)*sigma1 
  s2<-sqrt(1-rho^2)*sigma2
  # generate the chain
  X[1,]<-c(mu1,mu2)
  for(i in 2:N){
    x2<-X[i-1,2]
    m1<-mu1+rho*(x2-mu2)*sigma1/sigma2
    X[i,1]<-rnorm(1,m1,s1)
    x1<-X[i,1]
    m2<-mu2+rho*(x1-mu1)*sigma2/sigma1
    X[i,2]<-rnorm(1,m2,s2)
  }
  return(X)
}
gibbsr<-gibbsR(0,0,1,1,0.9,N)[(burn+1):N,]
plot(gibbsr,cex=0.5,main="Generated with pure R language",xlab = 'Xt',ylab = 'Yt')
```

```{r}
# Compare by `qqplot`
par(mfrow=c(1,2))
# compare sample Xt
qqplot(gibbsc[,1],gibbsr[,1],main="Xt",xlab = 'Rcpp',ylab = 'R')
qqline(gibbsc[,1],col = "red")
# compare sample Yt
qqplot(gibbsc[,2],gibbsr[,2],main="Yt",xlab = 'Rcpp',ylab = 'R')
qqline(gibbsc[,2],col = "red")
```


```{r}
#Compare the computation time of the two functions
ts<-microbenchmark(gibbsr<-gibbsR(0,0,1,1,0.9,N),
                   gibbsc<-gibbsC(0,0,1,1,0.9,N))
summary(ts)[,c(1,3,5,6)]
```
